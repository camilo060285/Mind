# Mind LLM Configuration
# Default provider is llama_cpp with phi model (fast local inference, no cloud costs)

# LLM Provider: llama_cpp, openai, anthropic, or ollama
MIND_LLM_PROVIDER=llama_cpp

# Model identifier: phi (recommended), qwen, gpt-4o-mini, claude-3-5-sonnet-latest, etc.
MIND_LLM_MODEL=phi

# llama.cpp binary path (default: ~/llama.cpp/build/bin/llama-completion)
# MIND_LLAMA_BIN=/home/username/llama.cpp/build/bin/llama-completion

# Models directory path (default: ~/local_llms/models)
# Should contain subdirectories: llm_a/model.gguf (phi), llm_b/model.gguf (qwen)
# MIND_MODELS_DIR=/home/username/local_llms/models

# Optional: Configure cloud providers if using them
# MIND_OPENAI_API_KEY=sk-...
# MIND_OPENAI_BASE_URL=https://api.openai.com/v1
# MIND_ANTHROPIC_API_KEY=sk-ant-...
# MIND_ANTHROPIC_BASE_URL=https://api.anthropic.com
# MIND_OLLAMA_BASE_URL=http://localhost:11434

# Local optimization parameters (tunable for different hardware)
# MIND_LLM_THREADS=2
# MIND_LLM_CONTEXT_SIZE=512
# MIND_LLM_TEMPERATURE=0.7
